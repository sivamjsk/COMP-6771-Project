{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# An Empirical Evaluation of Gray World and Neural Network Approaches for Image White Balancing\n",
    "### COMP  478/6771:   Image  Processing\n",
    "#### Submitted by:\n",
    "<ol> Ananya Varsha [40197012]\n",
    "<br> Sivakumaran Malli Janardhanan [40155790]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Necesary Code imports\n",
    "\n",
    "The below cell consists of all the necessary code imports that have been used throughout the process. It is a cleaner and more feasible approach to have all the imports on the top of the notebook rather than including it in individual cells"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "import tensorflow\n",
    "import keras\n",
    "import visualkeras\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, Concatenate, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras.layers import MaxPooling2D, Input, Conv2D, UpSampling2D, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "## print(\"Done importing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Static code variables declarations\n",
    "\n",
    "This cell consists of all the majore variable declarations and static data that will be later utlised in this notebook"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_name = \"awb\"\n",
    "# Size of the training images to be set before training\n",
    "img_size = 256\n",
    "\n",
    "#Array declartions for train/test images along with groung truth images\n",
    "train_images = []\n",
    "ground_truth_images = []\n",
    "use_saved_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Image collection\n",
    "\n",
    "This cell consists of the method to resize and append the images into an array. The images have been split into ground truth images and images for training, which are the tinted images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path=r\"images/Bad\"\n",
    "counter_train_image = 0;\n",
    "for image in os.listdir(path):\n",
    "    # filteration to avoid hidden file added by MacOS\n",
    "    if (image != \".DS_Store\"):\n",
    "        # Repeatedly iterate the folder to obtain all images\n",
    "        imgages = cv2.imread(os.path.join(path,image) ,cv2.IMREAD_COLOR)\n",
    "        # Resize the images for normalization\n",
    "        train_images_array = cv2.resize(imgages, (img_size, img_size))\n",
    "        train_images.append(train_images_array)\n",
    "        print(\"Saved train color image :\",counter_train_image)\n",
    "        counter_train_image+=1\n",
    "\n",
    "\n",
    "path=r\"images/Good\"\n",
    "counter_ground_image = 0;\n",
    "for image in os.listdir(path):\n",
    "    # filteration to avoid hidden file added by MacOS\n",
    "    if (image != \".DS_Store\"):\n",
    "        # Repeatedly iterate the folder to obtain all images\n",
    "        imgages = cv2.imread(os.path.join(path,image) ,cv2.IMREAD_COLOR)\n",
    "        # Resize the images for normalization\n",
    "        ground_images_array = cv2.resize(imgages, (img_size, img_size))\n",
    "        ground_truth_images.append(ground_images_array)\n",
    "        print(\"Saved ground image :\",counter_ground_image)\n",
    "        counter_ground_image+=1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Display dataset images\n",
    "\n",
    "The below cell picks 10 images and displays it in the cosole in a 2x5 grid\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to the folder containing the images\n",
    "folder_path = \"/Users/sivakumaran/Downloads/IP/COMP-6771-Project/images/Test\"\n",
    "\n",
    "# Load all image files in the folder\n",
    "image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "# Load the first 10 images\n",
    "images = [plt.imread(f) for f in image_files[:10]]\n",
    "\n",
    "# Create a new figure with a grid layout\n",
    "fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(10, 5))\n",
    "\n",
    "# Display the images in the grid\n",
    "for i, im in enumerate(images):\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    axs[row, col].imshow(im)\n",
    "    axs[row, col].axis('off')\n",
    "\n",
    "# Show the grid in the console\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalization & Split\n",
    "\n",
    "Before supplying the pictures to a Convolutional Neural Network (CNN), image normalisation should be carried out as a preparatory step. By guaranteeing that the input values are on a same scale, normalisation can make it simpler for the network to understand the underlying patterns in the data.  We used an 80-20 split, with 80% training data and 20% test data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_images = (np.array(train_images))/255\n",
    "ground_truth_images = (np.array(ground_truth_images))/255\n",
    "train_image_for_train, train_image_for_test, ground_truth_train, ground_truth_test = train_test_split(train_images, ground_truth_images, test_size=0.2)\n",
    "#print(len(train_image_for_train))\n",
    "#print(len(train_image_for_test))\n",
    "#print(len(ground_truth_train))\n",
    "#print(len(ground_truth_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset split visualization\n",
    "\n",
    "The below cell visualizes the data set both in terms of train/test split and the total images available for the Neural Network."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = {'0':len(train_image_for_train), \"1\":len(train_image_for_test), '2':len(ground_truth_train),\"3\":len(ground_truth_test)}\n",
    "images = list(data.keys())\n",
    "values = list(data.values())\n",
    "plt.bar(images, values, color ='maroon',width = 0.4)\n",
    "plt.xlabel(\"image split\")\n",
    "plt.ylabel(\"No. of images\")\n",
    "plt.title(\"Number of images for train/test\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Legend\\n\")\n",
    "print(\"0 - train_image_for_train \",len(train_image_for_train))\n",
    "print(\"1 - train_image_for_test \",len(train_image_for_test))\n",
    "print(\"2 - ground_truth_train \",len(ground_truth_train))\n",
    "print(\"3 - ground_truth_test \",len(ground_truth_test))\n",
    "\n",
    "labels = 'Training', 'Testing'\n",
    "\n",
    "sizes = [len(train_image_for_train) + len(ground_truth_train), len(train_image_for_test) + len(ground_truth_test)]\n",
    "explode = (0, 0.1)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct=\"%1.1f%%\",\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')\n",
    "plt.title(\"Divison of data\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Total Data set Size: \",len(train_image_for_train) + len(ground_truth_train) + len(train_image_for_test) + len(ground_truth_test))\n",
    "print(\"Train Image Size : \",len(train_image_for_train) + len(ground_truth_train) )\n",
    "print(\"Test Image Size : \",len(train_image_for_test) + len(ground_truth_test))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convolution Neural Network - UNet\n",
    "\n",
    "The bellow cell defined all the necessary layers for our Neural Network. Three channels of a 256x256 RGB picture are designated as the input layer. Four convolutional layers make up the encoder layers, and each is followed by a ReLU activation function. Each consecutive layer of the encoder doubles the number of filters to 48, 96, and 192 from the first layer's 24 to the last.\n",
    "\n",
    "The network's bottleneck is its bridge layers, which reduce the incoming data's dimensions for representation. A downsampling convolutional layer with 384 filters is used in this instance, and then an upsampling transpose convolutional layer with 192 filters.\n",
    "\n",
    "The three blocks that make up the decoder layers each have three convolutional layers with 96, 48, and 24 filters. A ReLU activation function follows each convolutional layer. Each block's output is up-sampled using a transpose convolutional layer with a stride of 2, doubling the input's spatial resolution.\n",
    "\n",
    "To combine the output of each decoder block with the corresponding encoder block, the concatenation operator is used. The final output of the network is an RGB image with the same spatial resolution as the input."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(256, 256, 3), name=\"Main Input\")\n",
    "\n",
    "# Define the encoder layers\n",
    "encoder_inc = Conv2D(24, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', name=\"encoder_inc\")(input_layer)\n",
    "encoder_down1 = Conv2D(48, kernel_size=(3,3), strides=(2,2), padding='same', activation='relu', name=\"encoder_down1\")(encoder_inc)\n",
    "encoder_down2 = Conv2D(96, kernel_size=(3,3), strides=(2,2), padding='same', activation='relu', name=\"encoder_down2\")(encoder_down1)\n",
    "encoder_down3 = Conv2D(192, kernel_size=(3,3), strides=(2,2), padding='same', activation='relu', name=\"encoder_down3\")(encoder_down2)\n",
    "\n",
    "# Define the bridge layers\n",
    "encoder_bridge_down = Conv2D(384, kernel_size=(3,3), strides=(2,2), padding='same', activation='relu', name=\"encoder_bridge_down\")(encoder_down3)\n",
    "encoder_bridge_up = Conv2DTranspose(192, kernel_size=(3,3), strides=(2,2), padding='same', activation='relu', name=\"encoder_bridge_up\")(encoder_bridge_down)\n",
    "\n",
    "#Define the decoder layers\n",
    "print(encoder_down3, encoder_bridge_up)\n",
    "concatenate_8 = Concatenate()([encoder_down3, encoder_bridge_up])\n",
    "sequential_8 = Sequential([\n",
    "    Conv2D(192, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'),\n",
    "    Conv2D(192, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'),\n",
    "    Conv2D(192, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu')\n",
    "], name=\"sequential_8\")(concatenate_8)\n",
    "conv2d_transpose_9 = Conv2DTranspose(96, kernel_size=(3,3), strides=(2,2), padding='same', activation='relu', name=\"conv2d_transpose_9\")(sequential_8)\n",
    "\n",
    "concatenate_9 = Concatenate()([encoder_down2, conv2d_transpose_9])\n",
    "sequential_9 = Sequential([\n",
    "    Conv2D(96, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'),\n",
    "    Conv2D(96, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'),\n",
    "    Conv2D(96, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu')\n",
    "], name=\"sequential_9\")(concatenate_9)\n",
    "conv2d_transpose_10 = Conv2DTranspose(48, kernel_size=(3,3), strides=(2,2), padding='same', activation='relu', name=\"conv2d_transpose_10\")(sequential_9)\n",
    "\n",
    "concatenate_10 = Concatenate()([encoder_down1, conv2d_transpose_10])\n",
    "sequential_10 = Sequential([\n",
    "    Conv2D(48, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'),\n",
    "    Conv2D(48, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'),\n",
    "    Conv2D(48, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu')\n",
    "], name=\"sequential_10\")(concatenate_10)\n",
    "conv2d_transpose_11 = Conv2DTranspose(24, kernel_size=(3,3), strides=(2,2), padding='same', activation='relu', name=\"conv2d_transpose_11\")(sequential_10)\n",
    "\n",
    "concatenate_11 = Concatenate()([input_layer, conv2d_transpose_11])\n",
    "sequential_11 = Sequential([\n",
    "    Conv2D(3, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'),\n",
    "    Conv2D(3, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'),\n",
    "    Conv2D(3, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu')\n",
    "], name=\"sequential_11\")(concatenate_11)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Training\n",
    "\n",
    "The below cell starts the training of a new model if it is not already preset. If a pre-trained model is present, the Keras pre-trained model will be utlised. The optimizer used here is Adam and the loss metric is Mean Squared Error. the number of epochs are 40 and the batch size is 32"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "model = Model(inputs=input_layer, outputs=sequential_11)\n",
    "model.compile(optimizer='Adam', loss='mse',metrics=['accuracy','MeanSquaredError','mae', 'mape'])\n",
    "if use_saved_model:\n",
    "    model = keras.models.load_model(\"awb.h5\")\n",
    "else:\n",
    "    print(\"Training new model\\n\")\n",
    "    model_history = model.fit(train_image_for_train, ground_truth_train, epochs=40, batch_size=32, validation_data=(train_image_for_test, ground_truth_test))\n",
    "if not use_saved_model:\n",
    "    model.save(\"awb_test.h5\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize the model using TensorFlow plot_model\n",
    "\n",
    "The below code will create a diagramatic representation of the Unet model that has been utilised"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#visualkeras.layered_view(model,legend=True,scale_xy= 1.5).show()\n",
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file=\"model.png\",\n",
    "    show_shapes=False,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    "    layer_range=None,\n",
    "    show_layer_activations=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Metrics\n",
    "\n",
    "The below cell will plot the essential metrics, like Mean Squared Error over epochs, Validation loss over epochs, and accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(model_history.epoch, model_history.history['loss'])\n",
    "plt.title('Epochs on Training Loss')\n",
    "plt.xlabel('# of Epochs')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(model_history.epoch, model_history.history['accuracy'])\n",
    "plt.title('Epochs on Training accuracy')\n",
    "plt.xlabel('# of Epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(model_history.epoch, model_history.history['val_loss'])\n",
    "plt.title('Epochs on validation loss')\n",
    "plt.xlabel('# of Epochs')\n",
    "plt.ylabel('validation loss')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Predict\n",
    "\n",
    "The below cell will be used to regenerate the new set of images from the testing split and store them in the resultant array"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = model.predict(train_image_for_test)\n",
    "model.evaluate(result, ground_truth_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting the image created by CNN\n",
    "\n",
    "The below cell plots the original image, tinted image and the image generated using CNN to allow us to compare the results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Ground Truth\")\n",
    "plt.imshow(ground_truth_test[1])\n",
    "ax = plt.gca()\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Tinted image\")\n",
    "plt.imshow(train_image_for_test[1])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Unet\")\n",
    "plt.imshow(result[1])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting the image created by Gray World method\n",
    "\n",
    "The below cell plots the original image, tinted image and the image generated using Gray World Method to allow us to compare the results. It also consists of the algorithm used in the Gray World approach."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img = cv2.imread(\"images/Bad/cat.4.jpg\")\n",
    "\n",
    "array = cv2.resize(img, (256, 256))\n",
    "array = (np.array(array))/255\n",
    "\n",
    "gw_result = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "avg_a = np.average(gw_result[:, :, 1])\n",
    "avg_b = np.average(gw_result[:, :, 2])\n",
    "gw_result[:, :, 1] = gw_result[:, :, 1] - ((avg_a - 128) )\n",
    "gw_result[:, :, 2] = gw_result[:, :, 2] - ((avg_b - 128) )\n",
    "gw_result = cv2.cvtColor(gw_result, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Tinted Image\")\n",
    "plt.imshow(train_image_for_test[2])\n",
    "plt.axis('off')\n",
    "#plt.show()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Gray world method\")\n",
    "plt.imshow(gw_result)\n",
    "plt.axis('off')\n",
    "#plt.show()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"CNN\")\n",
    "plt.imshow(result[2])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Peak Signal-to-Noise Ratio\n",
    "\n",
    "The below cell calcualtes the peak signal-to-noise ratio (PSNR) between the images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the original image and corrected image\n",
    "# Only for testing\n",
    "orig_img = cv2.imread('images/Good/cat.1.jpg')\n",
    "corr_img = cv2.imread('images/Bad/cat.1.jpg')\n",
    "\n",
    "# Convert the images to grayscale and resize them to the same size\n",
    "# Only for testing\n",
    "orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2GRAY)\n",
    "corr_img = cv2.cvtColor(corr_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "orig_img = cv2.resize(orig_img, (corr_img.shape[1], corr_img.shape[0]))\n",
    "\n",
    "#gw_result = cv2.resize(gw_result, (256, 256))\n",
    "# Compute the peak signal-to-noise ratio (PSNR) between the images\n",
    "mse = np.mean((train_image_for_test[2] - result[2]) ** 2)\n",
    "if mse == 0:\n",
    "    psnr = float('inf')\n",
    "else:\n",
    "    psnr = 20 * np.log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "# Print the PSNR value\n",
    "print('PSNR:', psnr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
